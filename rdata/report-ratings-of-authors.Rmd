---
title: "SCP wiki data analysis: Average ratings of authors"
output: 
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(contrasts = c("contr.sum", "contr.poly"))
```

```{r read}
# Data for all SCP articles
data1 <- read.table("data-3-09-21.txt", header=T)

# Data for author A
data2 <- read.table("author-data.txt", header=T)
```

```{r preprocess1, include=F}

scp1 <- as.integer(data1$scp_number)
rating1 <- as.integer(data1$rating)
comments1 <- as.integer(data1$comments)
```

```{r preprocess2, include=F}

scp2 <- c()

for (i in 0:length(data2$SCP)) {
  str <- data2$SCP[i]
  if (length(str) > 0 && startsWith(str, "SCP-")) {
    scp2 <- append(scp2, as.integer(substring(str, 5)))
  }
}

rating2 <- as.integer(data2$Rating)
comments2 <- as.integer(data2$Comments)
```

```{r preprocess3, include=F}

group <- c()
authora <- c()

for (i in 1:length(scp1)) {
  if (length(which(scp2 == scp1[i])) > 0) {
    group <- append(group, 2)
    authora <- append(authora, 1)
    
  } else {
    group <- append(group, 1)
    authora <- append(authora, 0)
  }
}

all.data <- data.frame(
  scp=scp1, 
  rating=rating1, 
  comments=comments1,
  group=as.factor(group),
  ratio=rating1 / (comments1 + 1),
  authorA=authora,
  comments2=comments1 ^ 2,
  comments3=comments1 ^ 3,
  comments4=comments1 ^ 4,
  comments5=comments1 ^ 5,
  comments6=comments1 ^ 6,
  comments7=comments1 ^ 7,
  comments8=comments1 ^ 8,
  comments9=comments1 ^ 9,
  comments10=comments1 ^ 10)

all.data <- subset(all.data, scp != 1)
group1 <- subset(all.data, group == 1)
group2 <- subset(all.data, group == 2)
```

# Summary

The SCP wiki is a community-driven creative writing website. Author A is one of my favourite authors on the site. The goal of this analysis is to determine whether their entries are more well-received by the general SCP wiki community than other authors' entries.

Three models were used. Each attempts to measure the significance of the difference between the popularity of articles belonging to author A and the popularity of articles not belonging to author A by estimating the attributes of a hypothetical random process for each entry. Two found a statistically significant difference. One failed to find such a difference, however this model was unsuitable for the analysis in the extreme. Therefore, the articles belonging to author A are probably more well-received by the general SCP wiki community than the other authors' entries.

## Results for model 1

The average rating of entries belonging to author A (average `r mean(group2$rating)`) is greater than the average rating of entries belonging to other authors (average `r mean(group1$rating)`). The difference is not significant (p-value $> 0.1$), though these results should be interpreted skeptically, due to a number of reasons that led to the model used being very unsuitable for the analysis:

* Negatively-rated entries are almost always removed from the site, causing the data to be skewed rightward.
* The skill level of different authors and the quality of their entries may vary.
* The rating of an entry depends on its visibility and quality. Some popular SCP entries may reference other entries, causing the visibility of one to depend on the visibility of another. As well, more skilled authors will tend to produce higher-quality entries.

## Results for model 2

The average rating-comment ratio of entries belonging to author A (average `r mean(group2$rating / (group2$comments + 1))`) is greater than the average rating-comment ratio of other authors (average `r mean(group1$rating / (group1$comments + 1))`). The difference is significant (p-value $< 0.001$). The model used for the analysis was unsuitable for the analysis, due to the right-skewness of the data caused by negatively-rated entries being removed from the site. Therefore these results should also be interpreted with some skepticism.

## Results for model 3

A linear model fit was done against multiple variables. The model notes the relationship between the number of comments on an article and the rating of the article. This model, while still not ideal, was more suitable than the previous two. The articles belonging to author A are rated more highly than articles belonging to other authors, and the difference is significant (p-value $< 0.001$).

# Calculations

## Preprocessing

Assign entries not belonging to author A to group 1, and entries belonging to author A to group 2. SCP-001 has been removed from the data, since it has multiple proposals, and the rating and comments recorded are for the hub page for the proposals, and do not correspond to any particular author's entry. To prevent division by 0, 1 was added to the denominator in the calculation of the rating-comment ratios. The variables `comments2` through `comments10` represent the higher-order terms `comments ^ 2`, `comments ^ 3`, through `comments ^ 10`.


```{r preprocess4}
head(all.data)
head(group1)
head(group2)
nrow(all.data) # Total number of observations
nrow(group1) # Number of replicates in group 1
nrow(group2) # Number of replicates in group 2
mean(group1$rating)
mean(group2$rating)
mean(group1$rating / (group1$comments + 1))
mean(group2$rating / (group2$comments + 1))
```


## Model 1

The model used is an unbalanced completely randomized design:

$$
\begin{aligned}
Y_{ij} &= \mu + \tau_i + R_{ij} & R_{ij} \sim N(0,\, \sigma^2)
\end{aligned}
$$

where

* $Y_{ij}$ is the response variable, corresponding to the rating of an entry
* $i=1, 2$ correspond to the treatment groups (entries not belonging to author A, and entries belonging to author A).
* $j=1, 2, \cdots, 5587$ are the replicates for group 1 (entries not belonging to author A), and $j=1,2,\cdots,110$ are the replicates for group 2 (entries belonging to author A)
* $\mu$ is the mean rating
* $\tau_1$ and $\tau_2$ are the treatment effects corresponding to group 1 (entries not belonging to author A) and group 2 (entries belonging to author A) respectively.

There is one constraint on the model: $0 = 5587\tau_1 + 110\tau_2$.

### Assessing appropriateness of model

```{r general}

model <- lm(rating~group, all.data)
sigmahat <- summary(model)$sigma
sigmahat # Residual standard error
plot(fitted(model), rstudent(model))
qqnorm(resid(model))
qqline(resid(model))
```

This is not an appropriate model. The assumptions of a linear model do not appear to be satisfied:

* The data is right-skewed, indicating that the distribution is probably not Normal. This is likely because negatively-rated entries are almost always removed from the site.

* The variance in the residuals for author A's entries appears to be less than the variance in the residuals for other authors' entries, so the constant variance assumption might not hold. A higher variance in the other authors' entries may be due to varying levels of skill among the other authors. This could be remedied by adding factor levels for other authors. However, the consistency of the quality of an author's entries likely differs between different authors, leading to different variances in the ratings of entries belonging to each author, so even if this were done, the constant variance assumption might still not hold.

* The ratings of each entry also may not be independent. The rating of an entry depends on its visibility and quality. Some popular SCP entries may reference other entries, causing the visibility of one to depend on the visibility of another. As well, more skilled authors will tend to produce higher-quality entries, which may be another source of dependence among the data.

### Hypothesis test

If we were to perform a hypothesis test anyway, we would be interested in the attribute $\theta = \tau_2 - \tau_1$, and the hypothesis $H_0: \theta \leq 0$ vs. $H_a: \theta > 0$. The estimator is $\tilde{\theta} = \tilde{\tau}_2 - \tilde{\tau}_1 = \frac{1}{r_2} \sum_{j=1}^{r_2} Y_{2j} - \frac{1}{r_1} \sum_{j=1}^{r_1} Y_{1j}$.

$$
\begin{aligned}
E(\tilde{\theta}) &= E \left( \frac{1}{r_2} \sum_{j=1}^{r_2} Y_{2j} - \frac{1}{r_1} \sum_{j=1}^{r_1} Y_{1j} \right) \\
&= \frac{1}{r_2} \sum_{j=1}^{r_2} E(Y_{2j}) - \frac{1}{r_1} \sum_{j=1}^{r_1} E(Y_{1j}) \\
&= \frac{1}{r_2} \sum_{j=1}^{r_2} (\mu + \tau_2) - \frac{1}{r_1} \sum_{j=1}^{r_1} (\mu + \tau_1) \\
&= \mu + \tau_2 - \mu - \tau_1 \\
&= \theta \\[10px]
Var(\tilde{\theta}) &= Var \left( \frac{1}{r_2} \sum_{j=1}^{r_2} Y_{2j} - \frac{1}{r_1} \sum_{j=1}^{r_1} Y_{1j} \right) \\
&= \frac{1}{r_2^2} \sum_{j=1}^{r_2} Var(Y_{2j}) + \frac{1}{r_1^2} \sum_{j=1}^{r_1} Var(Y_{1j}) \\
&= \frac{1}{5587} \sigma^2 + \frac{1}{110} \sigma^2
\end{aligned}
$$

$\tilde{\theta}$ is a linear combination of Normal random variables, so it's Normally distributed. Therefore, assuming $\theta = 0$,

$$
\begin{aligned}
\tilde{\theta} & \sim N \left(0,\, \left( \frac{1}{5587} + \frac{1}{110} \right) \sigma^2 \right) \\
\frac{\tilde{\theta}}{\sqrt{\left( \frac{1}{5587} + \frac{1}{110} \right) \sigma^2}} & \sim N(0,\,1) \\
\frac{\tilde{\theta}}{\sqrt{\left( \frac{1}{5587} + \frac{1}{110} \right) \tilde{\sigma}^2}} &\sim t_{n-3+1}
\end{aligned}
$$

where $n=5697$ is the number of observations; there are 3 non-$\sigma$ parameters and 1 constraint. The pivotal quantity is $$D = \frac{\tilde{\theta}}{\sqrt{\left( \frac{1}{5587} + \frac{1}{110} \right) \tilde{\sigma}^2}} \sim t_{5695}$$ The discrepancy statistic is $$d = \frac{\hat{\theta}}{\sqrt{\left( \frac{1}{5587} + \frac{1}{110} \right) \hat{\sigma}^2}}$$ The p-value would be $p = Pr(D > d)$.


```{r}
thetahat <- mean(group2$rating) - mean(group1$rating)
d <- thetahat / sqrt( (1 / 5587 + 1 / 110) * sigmahat ^ 2 )
pvalue <- 1 - pt(d, 5695)
pvalue
```
The p-value `r pvalue` is greater than $0.1$, so there is insufficient evidence against $H_0$. The difference between the average rating of author A's entries and the average rating of other entries is not statistically significant. Since the model is not an appropriate one, this result should be interpreted with skepticism.

## Model 2

The model used is the same as in model 1, aside from a difference in the interpretation of $\mu$ and the response variable $Y_{ij}$: they now correspond to the rating-comment ratio for an entry and the mean rating-comment ratio respectively.

### Assessing appropriateness of model

```{r}
model <- lm(ratio~group, all.data)
sigmahat <- summary(model)$sigma
sigmahat # Residual standard error
plot(fitted(model), rstudent(model))
qqnorm(resid(model))
qqline(resid(model))
```

The model is not suitable, mainly due to the right-skewness of the data, which indicates that the distribution of the residuals is probably not Normal. The plot of the residuals indicates that the expectation assumption ($E(R_{ij}) = 0$ for all $i$, $j$) also might not hold.

### Hypothesis test

If we were to perform a hypothesis test anyway, we would be interested in the attribute $\theta = \tau_2 - \tau_1$, and the hypothesis $H_0: \theta \leq 0$ vs. $H_a: \theta > 0$. The math is the same as before.

```{r}
thetahat <- mean(group2$ratio) - mean(group1$ratio)
d <- thetahat / sqrt( (1 / 5587 + 1 / 110) * sigmahat ^ 2 )
pvalue <- 1 - pt(d, 5695)
pvalue
```

The p-value `r pvalue` is less than $0.001$. There is very strong evidence against $H_0$. The difference between the rating-comment ratios of author A and other authors is significant. Due to the model not being appropriate, these results should be interpreted with skepticism.

## Model 3

```{r, include=F}
model.data <- subset(all.data, scp != 173)
model.data <- subset(model.data, scp != 682)
model.data <- subset(model.data, scp != 106)
model.data <- subset(model.data, scp != 49)
model.data <- subset(model.data, scp != 2521)
model.data <- subset(model.data, scp != 579)
```

A source of variance in the ratings is the visibility of the article -- that is, how well-known it is in the community. Articles that are seen by more people tend to be rated more highly. We can attempt to measure this by the number of comments on the article. A linear model fit was done to account for the visibility of the article. 

We use the model

$$
\begin{aligned}
Y = \beta_0 + \beta_1x_1 + &\beta_2x_2 \\
+ &\beta_3x_2^2 
+ \beta_4 x_2^3 
+ \beta_5 x_2 ^ 4 
+ \beta_6 x_2 ^ 5
+ \beta_7 x_2^6 
+ \beta_8 x_2^7 
+ \beta_9 x_2^8 
+ \beta_{10} x_2^9 
+ \beta_{11} x_2^{10} \\
+ &R, & R \sim N(0,\,\sigma^2)
\end{aligned}
$$

where

* $Y$ is the random response variable, representing the fourth-root of the rating
* $R$ is the random error
* $x_1 = 0$ for articles not belonging to author A, and $x_1 = 1$ for articles belonging to author A
* $x_2$ is the number of comments, included to account for the visibility of the article. Higher-order terms are added to improve the fit.

The transformation to the response variable was done to address a violation of the constant-variance assumption (funnel shape in residual plot, suggesting greater variance at higher fitted values).

The following SCP entries were removed from the data, due to being special situations in the wiki and for being extreme outliers in the data:

* SCP-173
* SCP-682
* SCP-106
* SCP-049
* SCP-579
* SCP-2521

The first five are early articles that are popular not for their quality, but for their legacy and importance to the lore. The sixth (SCP-2521) is not in a written format.

### Appropriateness of fit

```{r linear-model-2}
model <- lm(
  rating ^ 0.25 ~ 
    authorA + comments + comments2 + 
    comments3 + comments4 + comments5 + 
    comments6 + comments7 + comments8 + 
    comments9 + comments10, 
  model.data)
plot(fitted(model), rstudent(model))
qqnorm(resid(model))
qqline(resid(model))
```

The residuals mostly fall in a band around zero, indicating that the expectation assumption ($E(R) = 0$) is not obviously violated. The variance appears to be less at fitted values close to 0, indicating that the constant-variance assumption ($Var(R) = \sigma^2$) may not hold. The data is somewhat skewed left, but the skewness is less severe than in previous models. Multiple articles belonging to one author may still be a source of dependence.

### Assessing influential observations

```{r, echo=F}
plot(model, which=5)
```

The Cook's distance measures the influence of a particular observation on the model. None of the observations are significantly influential.

### Hypothesis test

We use the hypothesis test $H_0: \beta_1 = 0$ versus $H_a: \beta_1 \neq 0$.

```{r}
summary(model)
```

The estimate of the coefficient on `authorA` is $\hat{\beta}_1 =$ `r summary(model)$coefficients[2,1]` $> 0$, and the p-value for the hypothesis test is `r summary(model)$coefficients[2,4]` $< 0.001$. There is very strong evidence against $H_0$. The articles belonging to author A are rated more highly than articles belonging to other authors, and the difference is significant.